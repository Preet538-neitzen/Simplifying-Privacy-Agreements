{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MRC_BEPROJ.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1mYA2IPRuC2",
        "outputId": "7e83db53-bc70-4e86-9925-4a257de5fd99"
      },
      "source": [
        "cd /content/drive/MyDrive\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "woMgTCBw_oSn",
        "outputId": "348fd85d-af66-4b19-b023-9a8997389460"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-d6c65a1e1ff4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cd..\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omJntO7_SGvi",
        "outputId": "300400f3-1856-4d5b-df86-ba408a9881b1"
      },
      "source": [
        "!git clone https://github.com/wasiahmad/PolicyQA.git\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PolicyQA'...\n",
            "remote: Enumerating objects: 28, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 28 (delta 9), reused 9 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFH4pfHCSIU5",
        "outputId": "54798c5e-150c-4fbe-9b22-d4a11df05899"
      },
      "source": [
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsz9DYrcSJTZ",
        "outputId": "f8519493-8c8f-4fa7-e22d-d422b037b8c6"
      },
      "source": [
        "cd PolicyQA/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PolicyQA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4do5SMp5SMSv"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "\n",
        "# ============================================= PREPARING DATASET ======================================================\n",
        "class Sample:\n",
        "    def __init__(self, question, context, start_char_idx=None, answer_text=None, all_answers=None):\n",
        "        self.question = question\n",
        "        self.context = context\n",
        "        self.start_char_idx = start_char_idx\n",
        "        self.answer_text = answer_text\n",
        "        self.all_answers = all_answers\n",
        "        self.skip = False\n",
        "        self.start_token_idx = -1\n",
        "        self.end_token_idx = -1\n",
        "\n",
        "    def preprocess(self):\n",
        "        context = \" \".join(str(self.context).split())\n",
        "        question = \" \".join(str(self.question).split())\n",
        "        tokenized_context = tokenizer.encode(context)\n",
        "        tokenized_question = tokenizer.encode(question)\n",
        "        if self.answer_text is not None:\n",
        "            answer = \" \".join(str(self.answer_text).split())\n",
        "            end_char_idx = self.start_char_idx + len(answer)\n",
        "            if end_char_idx >= len(context):\n",
        "                self.skip = True\n",
        "                return\n",
        "            is_char_in_ans = [0] * len(context)\n",
        "            for idx in range(self.start_char_idx, end_char_idx):\n",
        "                is_char_in_ans[idx] = 1\n",
        "            ans_token_idx = []\n",
        "            for idx, (start, end) in enumerate(tokenized_context.offsets):\n",
        "                if sum(is_char_in_ans[start:end]) > 0:\n",
        "                    ans_token_idx.append(idx)\n",
        "            if len(ans_token_idx) == 0:\n",
        "                self.skip = True\n",
        "                return\n",
        "            self.start_token_idx = ans_token_idx[0]\n",
        "            self.end_token_idx = ans_token_idx[-1]\n",
        "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
        "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        if padding_length > 0:\n",
        "            input_ids = input_ids + ([0] * padding_length)\n",
        "            attention_mask = attention_mask + ([0] * padding_length)\n",
        "            token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        elif padding_length < 0:\n",
        "            self.skip = True\n",
        "            return\n",
        "        self.input_word_ids = input_ids\n",
        "        self.input_type_ids = token_type_ids\n",
        "        self.input_mask = attention_mask\n",
        "        self.context_token_to_char = tokenized_context.offsets\n",
        "\n",
        "\n",
        "def create_squad_examples(raw_data):\n",
        "    squad_examples = []\n",
        "    for item in raw_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            context = para[\"context\"]\n",
        "            for qa in para[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "                if \"answers\" in qa:\n",
        "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
        "                    all_answers = [_[\"text\"] for _ in qa[\"answers\"]]\n",
        "                    start_char_idx = qa[\"answers\"][0][\"answer_start\"]\n",
        "                    squad_eg = Sample(question, context, start_char_idx, answer_text, all_answers)\n",
        "                else:\n",
        "                    squad_eg = Sample(question, context)\n",
        "                squad_eg.preprocess()\n",
        "                squad_examples.append(squad_eg)\n",
        "    return squad_examples\n",
        "\n",
        "\n",
        "def create_inputs_targets(squad_examples):\n",
        "    dataset_dict = {\n",
        "        \"input_word_ids\": [],\n",
        "        \"input_type_ids\": [],\n",
        "        \"input_mask\": [],\n",
        "        \"start_token_idx\": [],\n",
        "        \"end_token_idx\": [],\n",
        "    }\n",
        "    for item in squad_examples:\n",
        "        if item.skip == False:\n",
        "            for key in dataset_dict:\n",
        "                dataset_dict[key].append(getattr(item, key))\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "    x = [dataset_dict[\"input_word_ids\"],\n",
        "         dataset_dict[\"input_mask\"],\n",
        "         dataset_dict[\"input_type_ids\"]]\n",
        "    y = [dataset_dict[\"start_token_idx\"], dataset_dict[\"end_token_idx\"]]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "\n",
        "class ValidationCallback(keras.callbacks.Callback):\n",
        "\n",
        "    def normalize_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        text = re.sub(regex, \" \", text)\n",
        "        text = \" \".join(text.split())\n",
        "        return text\n",
        "\n",
        "    def __init__(self, x_eval, y_eval):\n",
        "        self.x_eval = x_eval\n",
        "        self.y_eval = y_eval\n",
        "\n",
        "    def compute_f1(self,pred_tokens, truth_tokens):      \n",
        "        # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "        if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "            return int(pred_tokens == truth_tokens)\n",
        "        \n",
        "        common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "        \n",
        "        # if there are no common tokens then f1 = 0\n",
        "        if len(common_tokens) == 0:\n",
        "            return 0\n",
        "        \n",
        "        prec = len(common_tokens) / len(pred_tokens)\n",
        "        rec = len(common_tokens) / len(truth_tokens)\n",
        "        \n",
        "        return 2 * (prec * rec) / (prec + rec)\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        count = 0\n",
        "        f1_scores = []\n",
        "        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "            squad_eg = eval_examples_no_skip[idx]\n",
        "            offsets = squad_eg.context_token_to_char\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if start >= len(offsets):\n",
        "                continue\n",
        "            pred_char_start = offsets[start][0]\n",
        "            if end < len(offsets):\n",
        "                pred_char_end = offsets[end][1]\n",
        "                pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "            else:\n",
        "                pred_ans = squad_eg.context[pred_char_start:]\n",
        "            normalized_pred_ans = self.normalize_text(pred_ans)\n",
        "            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]\n",
        "            if normalized_pred_ans in normalized_true_ans:\n",
        "                count += 1\n",
        "            pred_tokens = normalized_pred_ans.split()\n",
        "            truth_tokens = [a.split() for a in normalized_true_ans]\n",
        "            f1_scores.append(max((self.compute_f1(pred_tokens, answer)) for answer in truth_tokens))\n",
        "        acc = count / len(self.y_eval[0])\n",
        "        print(f\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\")\n",
        "        print(f'\\nepoch={epoch +1}, f1 score = {sum(f1_scores)/len(f1_scores)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx1F-meoSOhs"
      },
      "source": [
        "train_path = 'data/train.json'\n",
        "eval_path = 'data/dev.json'\n",
        "with open(train_path) as f:\n",
        "  raw_train_data = json.load(f)\n",
        "with open(eval_path) as f:\n",
        "  raw_eval_data = json.load(f)\n",
        "max_seq_length = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w71-PqC4SRRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "736ba1c1-78da-46b8-b2ae-7ed9da28510c"
      },
      "source": [
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
        "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
        "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode(\"utf-8\")\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)\n",
        "train_squad_examples = create_squad_examples(raw_train_data)\n",
        "x_train, y_train = create_inputs_targets(train_squad_examples)\n",
        "print(f\"{len(train_squad_examples)} training points created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17056 training points created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV0l0A0l7xKB",
        "outputId": "aa272d61-e811-4ba5-aaf6-a8496b7df202"
      },
      "source": [
        "print(pooled_output.get_shape())\n",
        "print(sequence_output.get_shape())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 768)\n",
            "(None, 512, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igYywyG1uapu",
        "outputId": "745a4841-d24c-4cd0-847d-9d76d0959d17"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3gUrWIqSS-u",
        "outputId": "5547207b-7da0-4f99-ceff-536c94fbc0f6"
      },
      "source": [
        "eval_squad_examples = create_squad_examples(raw_eval_data)\n",
        "x_eval, y_eval = create_inputs_targets(eval_squad_examples)\n",
        "print(f\"{len(eval_squad_examples)} evaluation points created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3809 evaluation points created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO8T6fyRSTDb",
        "outputId": "dd304eb6-c2fc-4bd0-b136-9776b03ce3bd"
      },
      "source": [
        "\n",
        "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
        "start_logits = layers.Flatten()(start_logits)\n",
        "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
        "end_logits = layers.Flatten()(end_logits)\n",
        "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_type_ids (InputLayer)     [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 input_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "start_logit (Dense)             (None, 512, 1)       768         keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "end_logit (Dense)               (None, 512, 1)       768         keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 512)          0           start_logit[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 512)          0           end_logit[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 512)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 512)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,777\n",
            "Trainable params: 109,483,776\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc2qtXX9M5UH",
        "outputId": "0683e33f-1493-4a95-8a8a-d98c4e1692d4"
      },
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/PolicyQA/model_checkpoint/training_cp-{epoch:04d}.h5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=False,\n",
        "    mode='auto',\n",
        "    period=1\n",
        "    )\n",
        "\n",
        "# Save the weights using the `checkpoint_path` format\n",
        "model.save_weights(checkpoint_path.format(epoch=0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vlrygU2qdWX"
      },
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/PolicyQA/model_checkpoint/training_cp-0002.h5',custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "gJuhFqenSTFQ",
        "outputId": "713c67d7-922e-433f-f883-5a3c6fc28823"
      },
      "source": [
        "model.fit(x_train, y_train, epochs=1, batch_size=4, callbacks=[ValidationCallback(x_eval, y_eval),cp_callback])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3743/3743 [==============================] - ETA: 0s - loss: 2.8458 - activation_loss: 1.4129 - activation_1_loss: 1.4329"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4467b1593dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mValidationCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcp_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6f294b0f14d0>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpred_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_pred_ans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mtruth_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalized_true_ans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mf1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtruth_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-6f294b0f14d0>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpred_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_pred_ans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mtruth_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnormalized_true_ans\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mf1_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_f1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtruth_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nepoch={epoch + 1}, exact match score={acc:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: compute_f1() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbOvEDdXpScB"
      },
      "source": [
        "model.save('/content/drive/MyDrive/PolicyQA/models/model_checkpoint.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS87GwX-SXh5"
      },
      "source": [
        "# test_path='data/test.json'\n",
        "# with open(test_path) as f: raw_test_data = json.load(f)\n",
        "raw_test_data={\"data\": [{\"title\": \"\", \n",
        "          \"paragraphs\": \n",
        "          [{ \"context\": \"The Adobe Privacy Policy describes the privacy practices of Adobe apps and websites. If you are a resident of North America, your relationship is with Adobe Inc and the laws of California and the United States apply. If you reside outside of North America, your relationship is with Adobe Systems Software Ireland Limited, which is the controller with regard to your personal information collected by Adobe and the laws of Ireland apply. Please note that in order to use our apps and websites, you authorise Adobe to transfer your personal information across national borders and to other countries where Adobe and its partners operate, including the United States. The privacy protections and rights of authorities to access your information in these countries may not be equivalent to those in your country. We will only transfer your personal information to these countries where permitted to do so by law and we will take steps intended to ensure that your personal information continues to receive appropriate protections. If the content or information that you store on Adobe apps or websites contains personal information of other individuals, you must be legally permitted to share the personal information with Adobe. We will obtain your permission before sending you news and promotional material about Adobe, accessing information stored on your device relating to your use and engagement with, websites and apps and crash reports, and analysing your content. You can withdraw your consent to such activities at any time. This policy explains when we process personal information for our legitimate interests. You can ask us to stop processing this information. We use your personal information to enable you to register with Adobe and to provide you with our websites and apps and other products or services that you request. We provide interactive features that engage with social media sites, such as Facebook. If you use these features, these sites will send us personal information about you. We use cookies and other technologies to track the use of our websites and apps.. \", \n",
        "              \"qas\": \n",
        "            [{\"question\": \"What does this policy describe?\", \n",
        "              \"id\": \"43d0tj7wcdmhwadk\"},                          \n",
        "             {\"question\": \"What if I am a resident outside of America?\", \n",
        "              \"id\": \"knyp7n1i9r35ci82\"},           \n",
        "             {\"question\": \"Will they expose the data collected from me in social networking sites?\", \n",
        "              \"id\": \"6isrs6pl65f7ueuf\"},              \n",
        "              {\"question\": \"Are my websites and apps tracked by cookies?\", \n",
        "               \"id\": \"xslxbpslfpt535le\"}, \n",
        "             {\"question\": \"Do you follow confidentiality obligations?\", \n",
        "              \"id\": \"qnabo06neuot52m1\"\n",
        "             }\n",
        "            ]}]}]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgUk_4eISXkC",
        "outputId": "91769a9f-365b-4fed-863b-4250b93234ce"
      },
      "source": [
        "test_samples = create_squad_examples(raw_test_data)\n",
        "x_test, _ = create_inputs_targets(test_samples)\n",
        "pred_start, pred_end = model.predict(x_test)\n",
        "for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    test_sample = test_samples[idx]\n",
        "    offsets = test_sample.context_token_to_char\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    pred_ans = None\n",
        "    if start >= len(offsets):\n",
        "        continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "        pred_ans = test_sample.context[pred_char_start:offsets[end][1]]\n",
        "    else:\n",
        "        pred_ans = test_sample.context[pred_char_start:]\n",
        "    print(\"Q: \" + test_sample.question)\n",
        "    print(\"A: \" + pred_ans)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What does this policy describe?\n",
            "A: The Adobe Privacy Policy describes the privacy practices of Adobe apps and websites. If you are a resident of North America, your relationship is with Adobe Inc and the laws of California and the United States apply.\n",
            "Q: What if I am a resident outside of America?\n",
            "A: If you are a resident of North America, your relationship is with Adobe Inc and the laws of California and the United States apply. If you reside outside of North America\n",
            "Q: Will they expose the data collected from me in social networking sites?\n",
            "A: We will only transfer your personal information to these countries where permitted to do so by law and we will take steps intended to ensure that your personal information continues to receive appropriate protections.\n",
            "Q: Are my websites and apps tracked by cookies?\n",
            "A: cookies and other technologies\n",
            "Q: Do you follow confidentiality obligations?\n",
            "A: you authorise Adobe to transfer your personal information across national borders and to other countries where Adobe and its partners operate, including the United States. The privacy protections and rights of authorities to access your information in these countries may not be equivalent to those in your country. We will only transfer your personal information to these countries where permitted to do so by law and we will take steps intended to ensure that your personal information continues to receive appropriate protections.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9F3qIjjhsqi"
      },
      "source": [
        "raw_test_data={\"data\": [{\"title\": \"\", \n",
        "          \"paragraphs\": \n",
        "          [{ \"context\": \"Please take a moment to review some changes to our Terms and Data Policy . Your Instagram experience isnt changing, and you still own your photos and videos. We are giving you better ways to access your data and understand how its used. By continuing to use Instagram on or after July 14, 2018, you're agreeing to these updates. Instagram has been a part of Facebook since 2012, and we're making some corporate changes. Going forward, our Terms will reflect that Facebook Inc. is responsible for Instagram. The Instagram app and the way we process data are not changing. Our Terms are now more clear about the service we provide, and what we expect from every member of our community to keep Instagram a safe place for everyone. Here are some updates we want to make sure you know about: We updated our intellectual property licenses, but your rights aren't changing. You still own your photos and videos. We updated how we use information to show activity on Instagram, so people can see when you've interacted with an ad the same way we do on a regular post. We also have a new Data Policy that explains how data is collected, shared and used in the Facebook Products, including Instagram. The policy addresses newer features like stories, direct messaging, activity status and the creative tools in our cameras. We wanted to make sure you knew about this new information in the policy. We receive different kinds of information from your device, like how you tap and scroll, which can help distinguish humans from bots and detect fraud. We can use and share information for research, especially in ways that help us keep our community safe on Instagram, like to understand and prevent bullying and harassment. The policy has more information about what we collect from your activity and our partners, how we connect information across the Facebook Companies and how we personalize your experience, including ads. We provide ads without telling advertisers who you are. The policy has more information about what we do share with advertisers and partners. We never sell your data. Because the policy also covers Facebook, it includes information about facial recognition. We dont use facial recognition technology on Instagram. If we introduce it, we will let you know and give you a choice.\", \n",
        "              \"qas\": \n",
        "            [{\"question\": \"What would my cookies be used for?\", \n",
        "              \"id\": \"43d0tj7wcdmhwadk\"},              \n",
        "             {\"question\": \"Why would Instagram store my information?\",  \n",
        "              \"id\": \"hjwapte7oki8t3l5\"},              \n",
        "             {\"question\": \"Why would Instagram keep records about my browsing activity?\", \n",
        "              \"id\": \"knyp7n1i9r35ci82\"},           \n",
        "             {\"question\": \"Why would Instagram share my information with external organisations?\", \n",
        "              \"id\": \"6isrs6pl65f7ueuf\"},              \n",
        "              {\"question\": \"Why does Instagram collect my user name, language or my region?\", \n",
        "               \"id\": \"xslxbpslfpt535le\"}, \n",
        "             {\"question\": \"What kind of advertising would I receive by Instagram or its advertising partners?\", \n",
        "              \"id\": \"qnabo06neuot52m1\"\n",
        "             }\n",
        "            ]}]}]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbs7_2qmTI8d"
      },
      "source": [
        "def normalize_text(text):\n",
        "  text = text.lower()\n",
        "  text = \"\".join(ch for ch in text if ch not in set(string.punctuation))\n",
        "  regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "  text = re.sub(regex, \" \", text)\n",
        "  text = \" \".join(text.split())\n",
        "  return text\n",
        "\n",
        "def compute_f1(pred_tokens, truth_tokens):      \n",
        "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
        "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "    return int(pred_tokens == truth_tokens)\n",
        "  \n",
        "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "  # if there are no common tokens then f1 = 0\n",
        "  if len(common_tokens) == 0:\n",
        "    return 0\n",
        "  prec = len(common_tokens) / len(pred_tokens)\n",
        "  rec = len(common_tokens) / len(truth_tokens)\n",
        "  return 2 * (prec * rec) / (prec + rec)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdh0tPAgSbHa"
      },
      "source": [
        "def compute():\n",
        "  pred_start, pred_end = model.predict(x_eval)\n",
        "  count = 0\n",
        "  f1_scores = []\n",
        "  eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]\n",
        "  for idx, (start, end) in enumerate(zip(pred_start, pred_end)):\n",
        "    squad_eg = eval_examples_no_skip[idx]\n",
        "    offsets = squad_eg.context_token_to_char\n",
        "    start = np.argmax(start)\n",
        "    end = np.argmax(end)\n",
        "    if start >= len(offsets):\n",
        "      continue\n",
        "    pred_char_start = offsets[start][0]\n",
        "    if end < len(offsets):\n",
        "      pred_char_end = offsets[end][1]\n",
        "      pred_ans = squad_eg.context[pred_char_start:pred_char_end]\n",
        "    else:\n",
        "      pred_ans = squad_eg.context[pred_char_start:]\n",
        "    normalized_pred_ans = normalize_text(pred_ans)\n",
        "    normalized_true_ans = [normalize_text(_) for _ in squad_eg.all_answers]\n",
        "    if normalized_pred_ans in normalized_true_ans:\n",
        "      count += 1\n",
        "    pred_tokens = normalized_pred_ans.split()\n",
        "    truth_tokens = [a.split() for a in normalized_true_ans]\n",
        "    f1_scores.append(max((compute_f1(pred_tokens, answer)) for answer in truth_tokens))\n",
        "  acc = count / len(y_eval[0])\n",
        "  print(f\"\\nexact match score={acc:.2f}\")\n",
        "  print(f'\\nf1 score = {sum(f1_scores)/len(f1_scores)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJNkFWlaUU1G",
        "outputId": "98656255-367c-4819-c905-e764729fc5f0"
      },
      "source": [
        "compute()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "exact match score=0.29\n",
            "\n",
            "f1 score = 0.5518181564587945\n"
          ]
        }
      ]
    }
  ]
}